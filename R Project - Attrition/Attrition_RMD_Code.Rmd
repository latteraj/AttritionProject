---
title: "Attrition R Project"
author: "Rajanna Latte"
date: "09/08/2020"
output: html_document
---


## Libraries Used
```{r}
library(tidyverse)
library(data.table)
library(VIM)
library(DT)
library(gridExtra)
library(caret)
library(Metrics)
library(randomForest)
library(pROC)
library(e1071)
library(dtree)
library(corrplot)
library(DMwR)
library(tree)
```

## Set working directory
```{r}
setwd("D:/Imarticus Projects/RProject-Attrition/R Project - Attrition")
```


## Import Data
```{r}
attrition = fread("Attrition.csv", sep = ",", header = TRUE, stringsAsFactors = T)
```

## Understanding the data
```{r}
glimpse(attrition)
```
```{r}
summary(attrition)
```

```{r}
colnames(attrition)
```

In Attrition dataset, there are 1470 observation with 35 variables.Column Attrition has 1233 'NO' and 237 'Yes' that shows the unbalance data. We have to pay attention to the unbalance class algorithm problems.

Employee Count is equal 1 for all observation which can not generate useful value for the analysis. Hence, removing the Employee Count.

All employees are over 18. Hence, removing the column Over18, also the decision for Standard Hours is same.


Employee Number is a variable for identifying the specific employee.If we have more information about employee and the structure of the employee number, then we can extract some new features. But now it is not possible and we have to remove it from our data set.

More and more, we have to investigate that, how the company objective factors influence in attrition employees, and what kind of working environment most will cause employees attrition.


# Check Missing Values
```{r}
colSums(is.na(attrition))
```

```{r}
VIM::aggr(attrition)
```
#Must say, we are so lucky that this dataset has no missing values. 



Remove non value attributes.These variables can not play significant role because they are same for all records. Also, EmployeeNumber can be accepted as an indicator for the time of join to the company which can be used for new feature generation,But we do not have any meta data about it, then, lets remove it.


```{r}
attrition$EmployeeNumber <- NULL
attrition$StandardHours <- NULL
attrition$Over18 <- NULL
attrition$EmployeeCount <- NULL

cat("Data Set has ",dim(attrition)[1], " Rows and ", dim(attrition)[2], " Columns" )
```

## check for duplicated Record
```{r}
sum (is.na(duplicated(attrition)))
```
There are some variables that are categorical, but in the data set are integer. We have to change them to categorical. also, we do not need any dummy variable creation, where some machine learning algorithms like RF, XGBoost etc. can use categorical variables.

For other algorithms like NN we have to change categorical variable more than two level to dummy variable Variable with two level (Binary) can be change to number very easy.

```{r}
attrition$Education <- as.factor(attrition$Education)
attrition$EnvironmentSatisfaction <- as.factor(attrition$EnvironmentSatisfaction)
attrition$JobInvolvement <- as.factor(attrition$JobInvolvement)
attrition$JobLevel <- as.factor(attrition$JobLevel)
attrition$JobSatisfaction <- as.factor(attrition$JobSatisfaction)
attrition$PerformanceRating <- as.factor(attrition$PerformanceRating)
attrition$RelationshipSatisfaction <- as.factor(attrition$RelationshipSatisfaction)
attrition$StockOptionLevel <- as.factor(attrition$StockOptionLevel)
attrition$WorkLifeBalance <- as.factor(attrition$WorkLifeBalance)
```

## Vizualization

```{r}
attrition %>%
        group_by(Attrition) %>%
        tally() %>%
        ggplot(aes(x = Attrition, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Attrition", y="Attriation Counts")+
        ggtitle("Attrition Vizualization")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```

As we see here, 237/1470=0.16 % of the data label shows the "Yes" in Attrition. This problem should be handled during the process because unbalanced dataset will bias the prediction model towards the more common class (here is 'NO'). There are different approaches for dealing with unbalanced data in machine learning like using more data (here is not possible), Re-sampling , changing the machine performance metric, using various algorithms etc.

```{r}
ggplot(data=attrition, aes(attrition$Age)) + 
        geom_histogram(breaks=seq(20, 50, by=2), 
                       col="blue", 
                       aes(fill=..count..))+
        labs(x="Age", y="Count")+
        scale_fill_gradient("Count", low="red", high="green")
```
We could see the majority of employees are between 28-36 years. 34-36 years old are very popular.

```{r}
attrition %>%
        group_by(BusinessTravel) %>%
        tally() %>%
        ggplot(aes(x = BusinessTravel, y = n,fill=BusinessTravel)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Business Travel", y="Attriation Number")+
        ggtitle("Attrition according to the Business Travel")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))

attrition %>%
        group_by(BusinessTravel, Attrition) %>%
        tally() %>%
        ggplot(aes(x = BusinessTravel, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Business Travel", y="Attriation Number")+
        ggtitle("Attrition according to the Business Travel")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))

```
Here is the distribution of the data according to the Business Travel situation. more than 70% of employees travel rarely where just 10 % of them has no travel.

```{r}
attrition %>%
        ggplot(aes(x = BusinessTravel, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = 2) +
        labs(y = "Percentage", fill= "business Travel") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        labs(x="Business Travel", y="Percentage")+
        ggtitle("Attrition")
```
```{r}
attrition %>%
        group_by(Department) %>%
        tally() %>%
        ggplot(aes(x = Department, y = n,fill=Department)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.1, position = position_dodge(0.9))

attrition %>%
        group_by(Department, Attrition) %>%
        tally() %>%
        ggplot(aes(x = Department, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.1, position = position_dodge(0.9))
```

```{r}
attrition %>%
        ggplot(aes(x = Education, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = 2) +
        labs(y = "Percentage", fill= "Education") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")

attrition %>%
        group_by(Education, Attrition) %>%
        tally() %>%
        ggplot(aes(x = Education, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))+
        labs(x="Education", y="Attriation Number")+
        ggtitle("Attrition in regards to Education Level")
```

```{r}
attrition %>%
        ggplot(aes(x = Gender, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = -.5) +
        labs(y = "Percentage", fill= "Gender") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")
```
```{r}
attrition %>%
        ggplot(aes(x = MaritalStatus, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = -.5) +
        labs(y = "Percentage", fill= "MaritalStatus") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")
```

```{r}
attrition %>%
        ggplot(mapping = aes(x = MonthlyIncome)) + 
        geom_histogram(aes(fill = Attrition), bins=20)+
        labs(x="Monthly Income", y="Attriation Number")+
        ggtitle("Attrition with regards to Monthly Income")
```
```{r}
attrition %>%
        ggplot(aes(x = OverTime, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = 0.3) +
        labs(y = "Percentage", fill= "OverTime") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.3)) + 
        ggtitle("Attrition")


attrition %>%
        group_by(OverTime, Attrition) %>%
        tally() %>%
        ggplot(aes(x = OverTime, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.3, position = position_dodge(0.9))+
        labs(x="Over time", y="Attriation Number")+
        ggtitle("Attrition with regards to Over time")
```

```{r}
attrition%>%
        ggplot(aes(x = WorkLifeBalance, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = -.5) +
        labs(x = "Work Life Balance",y = "Percentage", fill= "WorkLifeBalance") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")

attrition %>%
        group_by(WorkLifeBalance, Attrition) %>%
        tally() %>%
        ggplot(aes(x = WorkLifeBalance, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))+
        labs(x="  Work Life Balance", y="Attriation Number")+
        ggtitle("Attrition with regards to  Work Life Balance")
```

## Using Raw data by RF

At the first Stage we use RF for getting some information about the prediction split Data to Train and Test

```{r}
rfData <- attrition
set.seed(123)
indexes = sample(1:nrow(rfData), size=0.8*nrow(rfData))
RFRaw.train.Data <- rfData[indexes,]
RFRaw.test.Data <- rfData[-indexes,]
```


## Model Building

```{r}
Raw.rf.model <- randomForest(Attrition~.,RFRaw.train.Data, importance=TRUE,ntree=1000)
```

```{r}
varImpPlot(Raw.rf.model)
```
As we see here, Over time, Age, MonthlyIncome, Jobrole and TotalWorkingYears are top five variables.

```{r}
Raw.rf.prd <- predict(Raw.rf.model, newdata = RFRaw.test.Data)
confusionMatrix(RFRaw.test.Data$Attrition, Raw.rf.prd)
```

```{r}
Raw.rf.plot<- plot.roc(as.numeric(RFRaw.test.Data$Attrition), as.numeric(Raw.rf.prd),lwd=2, type="b",print.auc=TRUE,col ="blue")
```

ACC = 0.8639 which is very good result but not enough measure. We see that the AUC is poor.

## Feature Engineering

Making Age Group 18-24 = Young , 25-54=Middle-Age , 54-120= Adult

```{r}
attrition$AgeGroup <- as.factor(
        ifelse(attrition$Age<=24,"Young", ifelse(
        attrition$Age<=54,"Middle-Age","Adult"
        ))
)
table(attrition$AgeGroup)
```

We could see the majority of employees are Young.


##### Total Satisfaction the total of the satisfaction from Job, Environment, etc.

```{r}
attrition$TotlaSatisfaction <- 
        as.numeric(attrition$EnvironmentSatisfaction)+
        as.numeric(attrition$JobInvolvement)+
        as.numeric(attrition$JobSatisfaction)+
        as.numeric(attrition$RelationshipSatisfaction)+
        as.numeric(attrition$WorkLifeBalance)

summary(attrition$TotlaSatisfaction)
```
Study Years for getting Education Level certificate

```{r}
table(attrition$Education)
```

```{r}
attrition$YearsEducation <-  ifelse(attrition$Education==1,10,ifelse(attrition$Education==2,12,
        ifelse(attrition$Education==3,16,ifelse(attrition$Education==4,18,22))))  

table(attrition$YearsEducation)
```
Majority of employees are 16 years education (Bachelor)


Less or more than average Monthly Income We calculate the average income and generate the level of income(High or Low)

```{r}
attrition$IncomeLevel <- as.factor(
        ifelse(attrition$MonthlyIncome<ave(attrition$MonthlyIncome),"Low","High")
)
table(attrition$IncomeLevel)
```

## Let's see the Correlation Matrix of Data

```{r}
corrplot(cor(sapply(attrition,as.integer)),method = "pie")
```

## New Random Forest

```{r}
rfData <- attrition
set.seed(123)
indexes = sample(1:nrow(rfData), size=0.8*nrow(rfData))
RFtrain.Data <- rfData[indexes,]
RFtest.Data <- rfData[-indexes,]

rf.model <- randomForest(Attrition~.,RFtrain.Data, importance=TRUE,ntree=500)
varImpPlot(rf.model)
```
Here we see: OverTime, TotalSatisfaction (Generated Attr.), MonthlyIncome, Age and TotalWorkingYears are top five variables.

```{r}
rf.prd <- predict(rf.model, newdata = RFtest.Data)
confusionMatrix(RFtest.Data$Attrition, rf.prd)
```

```{r}
rf.Plot<- plot.roc (as.numeric(RFtest.Data$Attrition), as.numeric(rf.prd),lwd=2, type="b", print.auc=TRUE,col ="blue")
```

It is better than FR algorithm with raw data. TotalSatisfaction is high important attribute.

## Using Support Vector Machine

```{r}
svmData <- attrition
set.seed(123)
indexes = sample(1:nrow(svmData), size=0.8*nrow(svmData))
SVMtrain.Data <- svmData[indexes,]
SVMtest.Data <- svmData[-indexes,]
tuned <- tune(svm,factor(Attrition)~.,data = SVMtrain.Data)
svm.model <- svm(SVMtrain.Data$Attrition~., data=SVMtrain.Data
                 ,type="C-classification", gamma=tuned$best.model$gamma
                 ,cost=tuned$best.model$cost
                 ,kernel="radial")
svm.prd <- predict(svm.model,newdata=SVMtest.Data)
confusionMatrix(svm.prd,SVMtest.Data$Attrition)
```

```{r}
svm.plot <-plot.roc (as.numeric(SVMtest.Data$Attrition), as.numeric(svm.prd),lwd=2, type="b", print.auc=TRUE,col ="blue")
```

as we see, in compare to RF, Accuracy is lower to 0.8503 and AUC to 0.542 which is not better than RF. And there is no False Negative! and a lot of False Positive!


## Decision Tree

Here Dtree will be investigated and compared to other approaches. DTree selected because it is a very good algorithm for interpretion for non-technical like HR.

```{r}
DtData <- attrition
set.seed(123)
indexes = sample(1:nrow(DtData), size=0.8*nrow(DtData))
DTtrain.Data <- DtData[indexes,]
DTtest.Data <- DtData[-indexes,]

dtree.model <- tree::tree (Attrition ~., data = DTtrain.Data)
plot(dtree.model)
text(dtree.model, all = T)
```

```{r}
dtree.prd <- predict(dtree.model, DTtest.Data, type = "class")
confusionMatrix(dtree.prd,DTtest.Data$Attrition)
```

```{r}
dtree.plot <- plot.roc (as.numeric(DTtest.Data$Attrition), as.numeric(dtree.prd),lwd=2, type="b", print.auc=TRUE, col ="blue")
```

not very nice result! Accuracy is 0.8605 where AUC is 0.648 which is better than SVM Always we can not get better result from RF instead of DTree, but here it is a nice result.

## Exterme Gradient Boost

```{r}
set.seed(123)
xgbData <- attrition
indexes <- sample(1:nrow(xgbData), size=0.8*nrow(xgbData))
XGBtrain.Data <- xgbData[indexes,]
XGBtest.Data <- xgbData[-indexes,]

formula = Attrition~.
fitControl <- trainControl(method="cv", number = 3,classProbs = TRUE )
xgbGrid <- expand.grid(nrounds = 50,
                       max_depth = 12,
                       eta = .03,
                       gamma = 0.01,
                       colsample_bytree = .7,
                       min_child_weight = 1,
                       subsample = 0.9
)
```

```{r}
XGB.model <- train(formula, data = XGBtrain.Data,
                  method = "xgbTree"
                  ,trControl = fitControl
                  , verbose=0
                  , maximize=FALSE
                  ,tuneGrid = xgbGrid
)
```

```{r}
importance <- varImp(XGB.model)
varImportance <- data.frame(Variables = row.names(importance[[1]]), 
                            Importance = round(importance[[1]]$Overall,2))
# Create a rank variable based on importance of variables
rankImportance <- varImportance %>%
        mutate(Rank = paste0('#',dense_rank(desc(Importance))))
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
                           y = Importance)) +
        geom_bar(stat='identity',colour="white", fill = "lightgreen") +
        geom_text(aes(x = Variables, y = 1, label = Rank),
                  hjust=0, vjust=.5, size = 4, colour = 'black',
                  fontface = 'bold') +
        labs(x = 'Variables', title = 'Relative Variable Importance') +
        coord_flip() + 
        theme_bw()
```

As we see above: MonthlyIncome, DailyRate, OvertimeYes, TotalSatisfaction and Age are top five attributes.


```{r}
XGB.prd <- predict(XGB.model,XGBtest.Data)
confusionMatrix(XGB.prd, XGBtest.Data$Attrition)
```

```{r}
XGB.plot <- plot.roc (as.numeric(XGBtest.Data$Attrition), as.numeric(XGB.prd),lwd=2, type="b", print.auc=TRUE,col ="blue")
```

As we see the ACC is 0.8776 which is good result. Perfect: the best result is in Balanced Accuracy which is 0.6418


## Solving Unbalanced data problem

As we mentioned before there is unbalanced problem in class lable. There are some approaches to solve the problem. here we use SMOT method.

```{r}
Classcount = table(attrition$Attrition)
# Over Sampling
over = ( (0.6 * max(Classcount)) - min(Classcount) ) / min(Classcount)
# Under Sampling
under = (0.4 * max(Classcount)) / (min(Classcount) * over)

over = round(over, 1) * 100
under = round(under, 1) * 100
#Generate the balanced data set

BalancedData = SMOTE(Attrition~., attrition, perc.over = over, k = 5, perc.under = under)
# let check the output of the Balancing
BalancedData %>%
        group_by(Attrition) %>%
        tally() %>%
        ggplot(aes(x = Attrition, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Attrition", y="Attriation Count")+
        ggtitle("Attrition")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```
## Let's try XGBoost with the Balanced Data

```{r}
set.seed(153)
xgbData <- BalancedData
indexes = sample(1:nrow(xgbData), size=0.8*nrow(xgbData))
BLtrain.Data <- xgbData[indexes,]
BLtest.Data <- xgbData[-indexes,]

formula = Attrition~.
fitControl <- trainControl(method="cv", number = 3,classProbs = TRUE )
xgbGrid <- expand.grid(nrounds = 500,
                       max_depth = 20,
                       eta = .03,
                       gamma = 0.01,
                       colsample_bytree = .7,
                       min_child_weight = 1,
                       subsample = 0.9
)

XGB.model = train(formula, data = BLtrain.Data,
                  method = "xgbTree"
                  ,trControl = fitControl
                  , verbose=0
                  , maximize=FALSE
                  ,tuneGrid = xgbGrid
                  ,na.action = na.pass
)
```

```{r}
importance <- varImp(XGB.model)
varImportance <- data.frame(Variables = row.names(importance[[1]]), 
                            Importance = round(importance[[1]]$Overall,2))
# Create a rank variable based on importance
rankImportance <- varImportance %>%
        mutate(Rank = paste0('#',dense_rank(desc(Importance))))
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
                           y = Importance)) +
        geom_bar(stat='identity',colour="white", fill = "lightgreen") +
        geom_text(aes(x = Variables, y = 1, label = Rank),
                  hjust=0, vjust=.5, size = 4, colour = 'black',
                  fontface = 'bold') +
        labs(x = 'Variables', title = 'Relative Variable Importance') +
        coord_flip() + 
        theme_bw()
```

```{r}
NewXGB.prd <- predict(XGB.model,BLtest.Data)
confusionMatrix(NewXGB.prd, BLtest.Data$Attrition)
```

```{r}
XGB.plot <- plot.roc (as.numeric(BLtest.Data$Attrition), as.numeric(NewXGB.prd),lwd=2, type="b", print.auc=TRUE, col ="blue")
```
## Final Results

Accuracy : 0.9156
AUC about 0.912


## Plotting all models in one screen

```{r}
par(mfrow=c(2,3))
plot.roc (as.numeric(XGBtest.Data$Attrition), as.numeric(XGB.prd),main="XGBoost",lwd=2, type="b", print.auc=TRUE, col ="blue")
plot.roc (as.numeric(DTtest.Data$Attrition), as.numeric(dtree.prd), main="DTree",lwd=2, type="b", print.auc=TRUE, col ="brown")
plot.roc (as.numeric(BLtest.Data$Attrition), as.numeric(NewXGB.prd),main="New XGBoost",lwd=2, type="b", print.auc=TRUE, col ="green")
plot.roc (as.numeric(SVMtest.Data$Attrition), as.numeric(svm.prd),main="SVM",lwd=2, type="b", print.auc=TRUE, col ="red")
plot.roc (as.numeric(RFRaw.test.Data$Attrition), as.numeric(Raw.rf.prd), main="Random Forest",lwd=2, type="b", print.auc=TRUE, col ="seagreen4")
plot.roc (as.numeric(RFtest.Data$Attrition), as.numeric(rf.prd), main="Raw Data Random Forest",lwd=2, type="b", print.auc=TRUE, col ="seagreen")
```

## Thank You,
